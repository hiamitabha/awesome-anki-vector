{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "kernel-vector-sign-language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiamitabha/awesome-anki-vector/blob/master/kernel_vector_sign_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PihERIiMpiek",
        "colab_type": "text"
      },
      "source": [
        "Choose _DEVICE out of CPU, GPU, TPU.\n",
        "\n",
        "Choose _MODEL out of Convolution Neural Network (CNN) or RESNET (Resnet-50 available in Keras)\n",
        "\n",
        "Choose _OPTIMZER out of all the optimizers available in Keras: SGD, RMSprop, Adagrad, Adadelta, Adam, AdaMax, Nadam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jARBqQIbIlVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_DEVICE = \"GPU\"\n",
        "_MODEL = \"RESNET\"\n",
        "_OPTIMIZER = \"SGD\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TQq__pxBdZa",
        "colab_type": "text"
      },
      "source": [
        "Set up our run time environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZfX4A_k1cyB",
        "colab_type": "code",
        "outputId": "e9ceec6a-2e34-405e-f981-b217b7a4f1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "def get_gpu_device_name():\n",
        "    \"\"\"\n",
        "        Get the device name of a GPU device\n",
        "    \"\"\"\n",
        "    device_name = tf.test.gpu_device_name()\n",
        "    if device_name != '/device:GPU:0':\n",
        "        print (device_name) \n",
        "        raise SystemError('GPU device not found')\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "    #GPU count and name\n",
        "    !nvidia-smi -L\n",
        "    return device_name\n",
        "\n",
        "def get_tpu_strategy():\n",
        "    \"\"\"\n",
        "        Get the tpu_strategy for a TPU\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    except ValueError:\n",
        "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    return tpu_strategy\n",
        "\n",
        "if _DEVICE == \"TPU\":\n",
        "    device_name = 'tpu'\n",
        "    tpu_strategy = get_tpu_strategy()\n",
        "elif _DEVICE == \"GPU\":\n",
        "    device_name = get_gpu_device_name()\n",
        "elif _DEVICE == \"CPU\":\n",
        "    device_name = '/cpu:0'\n",
        "else:\n",
        "    print (\"Incorrect device option\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0-rc2\n",
            "Found GPU at: /device:GPU:0\n",
            "GPU 0: Tesla K80 (UUID: GPU-4f72444d-e833-cb78-4c19-f47de68116c5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6MhmZFU1cx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import tensorflow.keras\n",
        "    from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
        "    from tensorflow.keras.models import Sequential, Model, model_from_json\n",
        "    from tensorflow.keras.preprocessing.image import img_to_array\n",
        "    from tensorflow.keras import applications\n",
        "    from tensorflow.keras.callbacks import EarlyStopping\n",
        "except ImportError as exc:\n",
        "    sys.exit(\"No keras\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "B6KdzfIl1cx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scXX6GW91cx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    sys.exit(\"Cannot import from PIL: Do `pip3 install --user Pillow` to install\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AqIBytu1cyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "except ImportError as exc:\n",
        "    sys.exit(\"Cannot import scikit\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E4o-QLv1cyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetworkConstants():  # pylint: disable=too-few-public-methods\n",
        "    \"\"\"Constant values used as image and network parameters.\"\"\"\n",
        "\n",
        "    # Width of images passed to the network\n",
        "    IMAGE_WIDTH: int = 200\n",
        "\n",
        "    # Height of images passed to the network\n",
        "    IMAGE_HEIGHT: int = 200\n",
        "\n",
        "    # Currently set to 2 alphabet images and 1 background image class\n",
        "    # Number of classes that the network can categorize\n",
        "    NUM_CLASSES: int = 27\n",
        "\n",
        "    # The fraction of images passed to the network during training that should\n",
        "    # be used as a validation set. Range: 0 to 1\n",
        "    VALIDATION_SPLIT: float = 0.2\n",
        "\n",
        "    # The fraction of images passed to the network during training that should\n",
        "    # be used as a test set. Range: 0 to 1\n",
        "    TEST_SPLIT: float = 0.1\n",
        "\n",
        "    # Number of epochs on which to train the network\n",
        "    EPOCHS: int = 20\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "zp3bD_-81cyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import io\n",
        "class SignLanguageRecognizer():\n",
        "    \"\"\"Recognize sign language hand signals using Vector's camera feed.\n",
        "\n",
        "    A convolutional neural network is used to predict the hand signs.\n",
        "    The network is built with a Keras Sequential model with a TensorFlow backend.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        device_name = tf.test.gpu_device_name()\n",
        "        if device_name != '/device:GPU:0':\n",
        "            print(\n",
        "               '\\n\\nThis error most likely means that this notebook is not '\n",
        "               'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "               'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "            raise SystemError('GPU device not found')\n",
        "        \"\"\"\n",
        "        global _OPTIMIZER\n",
        "        self.training_images: np.ndarray = None\n",
        "        self.training_labels: np.ndarray = None\n",
        "        self.test_images: np.ndarray = None\n",
        "        self.test_labels: np.ndarray = None\n",
        "        self.model: tf.keras.engine.sequential.Sequential = None\n",
        "        self.graph: tf.python.framework.ops.Graph = tf.compat.v1.get_default_graph()\n",
        "        # List of optimizers and their default values chosen from Keras documentation at\n",
        "        # https://keras.io/optimizers/\n",
        "        if _OPTIMIZER == \"SGD\":\n",
        "            self.optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        elif _OPTIMIZER == \"Adam\":\n",
        "            self.optimizer = tf.keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
        "        elif _OPTIMIZER == \"RMSprop\":\n",
        "            self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
        "        elif _OPTIMIZER == \"Adagrad\":\n",
        "            self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
        "        elif _OPTIMIZER == \"Adadelta\":\n",
        "            self.optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)\n",
        "        elif _OPTIMIZER == \"Adamax\":\n",
        "            self.optimizer = keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
        "        elif _OPTIMIZER == \"Nadam\":\n",
        "            self.optimizer = keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
        "            \n",
        "    def load_datasets(self, datasetFile) -> None:\n",
        "        \"\"\"Load the training and test datasets required to train the model.\n",
        "  \n",
        "        \"\"\"\n",
        "\n",
        "        if not dataset:\n",
        "            sys.exit(\"Cannot load dataset.\")\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "        countLabels = dict()\n",
        "        count = 0\n",
        "        with ZipFile(datasetFile) as archive:\n",
        "            for entry in archive.infolist():\n",
        "                filename = entry.filename.split('/')[-1]\n",
        "                if filename.endswith(\".png\") and not filename.startswith(\".\"):\n",
        "                   count += 1\n",
        "                   #if (count % 4 == 0):\n",
        "                   #    continue\n",
        "                   image_data = archive.read(entry.filename)\n",
        "                   data = io.BytesIO(image_data)\n",
        "                   image = Image.open(data)\n",
        "                   if image:    \n",
        "                       # Convert image to an array with shape (image_width, image_height, 1)\n",
        "                       image_data = img_to_array(image)\n",
        "                       images.append(image_data)\n",
        "\n",
        "                       if filename.startswith(\"background\"):\n",
        "                           # Use the last class to denote an unknown/background image\n",
        "                           label = NetworkConstants.NUM_CLASSES - 1\n",
        "                       else:\n",
        "                           # Use ordinal value offsets to denote labels for all alphabets\n",
        "                           label = ord(filename[0]) - 97\n",
        "\n",
        "                       labels.append(label)\n",
        "                       if countLabels.get(chr(label + 97)) is not None:\n",
        "                           countLabels[chr(label + 97)] += 1\n",
        "                       else:\n",
        "                           countLabels[chr(label + 97)] = 1\n",
        "                   else:\n",
        "                       print (\"Not using this file\")\n",
        "                       continue\n",
        "                   \n",
        "\n",
        "        print (\"Here is a count of labels from our repository\")\n",
        "        print (countLabels)\n",
        "        # Normalize the image data\n",
        "        images = np.array(images, dtype=\"float\") / 255.0\n",
        "        # Convert labels to a numpy array\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Split data read in to training and test segments\n",
        "        self.training_images, self.test_images, self.training_labels, self.test_labels = train_test_split(images, labels, \n",
        "                                                                                                          test_size=NetworkConstants.TEST_SPLIT)\n",
        "\n",
        "        # Convert array of labels in to binary classs matrix\n",
        "        self.training_labels = tf.keras.utils.to_categorical(self.training_labels, num_classes=NetworkConstants.NUM_CLASSES)\n",
        "        self.test_labels = tf.keras.utils.to_categorical(self.test_labels, num_classes=NetworkConstants.NUM_CLASSES)\n",
        "    \n",
        "    def create_cnn_model(self) -> None:\n",
        "        \"\"\"Creates a convolutional neural network model with the following architecture:\n",
        "\n",
        "        ConvLayer -> MaxPoolLayer -> ConvLayer -> MaxPoolLayer -> ConvLayer ->\n",
        "        Dropout -> Flatten -> Dense -> Dropout -> Dense\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            recognizer = SignLanguageRecognizer()\n",
        "            recognizer.load_datasets(\"/path/to/dataset_root_folder\")\n",
        "            recognizer.create_model()\n",
        "        \"\"\"\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(NetworkConstants.IMAGE_WIDTH, \n",
        "                                                                                      NetworkConstants.IMAGE_HEIGHT, 1)))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        self.model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        self.model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
        "\n",
        "        self.model.add(Dropout(0.25))\n",
        "        self.model.add(Flatten())\n",
        "\n",
        "        self.model.add(Dense(128, activation=\"relu\"))\n",
        "        self.model.add(Dropout(0.5))\n",
        "        self.model.add(Dense(NetworkConstants.NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "        \n",
        "        self.model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                            optimizer=self.optimizer,\n",
        "                            metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "    \n",
        "    def create_resnet_model(self) -> None:\n",
        "        base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape=(NetworkConstants.IMAGE_WIDTH, \n",
        "                                                                                      NetworkConstants.IMAGE_HEIGHT, 1))\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.25)(x)\n",
        "        predictions = Dense(NetworkConstants.NUM_CLASSES, activation= 'softmax')(x)\n",
        "        self.model = Model(inputs = base_model.input, outputs = predictions)\n",
        "        self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        self.model.summary()\n",
        "\n",
        "    def train_model(self, epochs: int = NetworkConstants.EPOCHS, verbosity: int = 1) -> None:\n",
        "        \"\"\"Trains the model off of the training and test data provided\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            recognizer = SignLanguageRecognizer()\n",
        "            recognizer.load_datasets(\"/path/to/dataset_root_folder\")\n",
        "            recognizer.create_model()\n",
        "            recognizer.train_model()\n",
        "        \"\"\"\n",
        "\n",
        "        if self.training_images.size == 0 or self.training_labels.size == 0:\n",
        "            sys.exit(\"Training dataset is empty. Build a dataset with `data_gen.py` before training the model.\")\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "        history = self.model.fit(self.training_images,\n",
        "                       self.training_labels,\n",
        "                       epochs=epochs,\n",
        "                       verbose=verbosity,\n",
        "                       validation_split=NetworkConstants.VALIDATION_SPLIT,\n",
        "                       callbacks=[es])\n",
        "        return history\n",
        "      \n",
        "    \n",
        "    def save_model(self, model_config_filename: str, model_weights_filename: str) -> None:\n",
        "        \"\"\"Saves a model's config and weights for latter use.\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            recognizer = SignLanguageRecognizer()\n",
        "            recognizer.load_datasets(args.dataset_root_folder)\n",
        "            recognizer.create_model()\n",
        "            recognizer.train_model()\n",
        "            recognizer.save_model(\"/path/to/model_config_filename\", \"/path/to/model_weights_filename\")\n",
        "        \"\"\"\n",
        "        json_model = self.model.to_json()\n",
        "        # Save the network architecture\n",
        "        with open(model_config_filename, \"w\") as file:\n",
        "            file.write(json_model)\n",
        "        # Save the model's assigned weights\n",
        "        self.model.save_weights(model_weights_filename)\n",
        "\n",
        "    def printme(self):\n",
        "        print (self.training_images[0])\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG8I6zEjY7IZ",
        "colab_type": "code",
        "outputId": "b7dbd607-d9db-49a5-a9de-09a013be05a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coQlecBqZIcn",
        "colab_type": "code",
        "outputId": "b0948811-8731-4479-d964-dc4bd4cac002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "dataset = '/content/drive/My Drive/training-a-robot-to-understand-sign-language.zip'\n",
        "stats = os.stat(dataset)\n",
        "\n",
        "print (stats)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "os.stat_result(st_mode=33152, st_ino=70, st_dev=77, st_nlink=1, st_uid=0, st_gid=0, st_size=133773677, st_atime=1585121187, st_mtime=1585034302, st_ctime=1585034302)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEr_08NcsleB",
        "colab_type": "code",
        "outputId": "dd99c2c1-0062-471d-9f17-33d690c4424d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import timeit\n",
        "recognizer = SignLanguageRecognizer()\n",
        "recognizer.load_datasets(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is a count of labels from our repository\n",
            "{'a': 380, 'b': 490, '{': 460, 'c': 380, 'd': 290, 'e': 270, 'f': 340, 'g': 300, 'h': 350, 'i': 350, 'k': 360, 'l': 330, 'm': 610, 'n': 340, 'o': 370, 'p': 260, 'q': 180, 'r': 320, 's': 230, 't': 260, 'u': 329, 'v': 309, 'w': 319, 'x': 279, 'y': 370}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDeyHK3bz7-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_evaluate(recognizer, model=\"CNN\"):\n",
        "    if model == \"RESNET\":\n",
        "        print (\"Building RESNET\")\n",
        "        recognizer.create_resnet_model()\n",
        "    else:\n",
        "        recognizer.create_cnn_model()\n",
        "    starttime = timeit.default_timer()\n",
        "    history = recognizer.train_model()\n",
        "    print(\"The time difference is :\", timeit.default_timer() - starttime)\n",
        "    test_score = recognizer.model.evaluate(recognizer.test_images, recognizer.test_labels, verbose=1)\n",
        "    print(f\"{recognizer.model.metrics_names[1].capitalize()}: {test_score[1] * 100}%\")\n",
        "    return history\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTlb4BY1cyL",
        "colab_type": "code",
        "outputId": "5a73a43f-68e9-4dcc-a3c5-5e335a0cb5b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "if _DEVICE == \"TPU\":\n",
        "    with tpu_strategy.scope():\n",
        "        history = train_and_evaluate(recognizer, _MODEL)\n",
        "elif _DEVICE == \"GPU\":\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = train_and_evaluate(recognizer, _MODEL)\n",
        "elif _DEVICE == \"CPU\":\n",
        "    with tf.device('/cpu:0'):\n",
        "        history = train_and_evaluate(recognizer, _MODEL)\n",
        "else:\n",
        "    print (\"Nothing to do!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building RESNET\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 200, 200, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 206, 206, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 100, 100, 64) 3200        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 100, 100, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 100, 100, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 102, 102, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 50, 50, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 50, 50, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 50, 50, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 50, 50, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 50, 50, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 50, 50, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 50, 50, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 50, 50, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 50, 50, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 50, 50, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 50, 50, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 50, 50, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 50, 50, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 50, 50, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 50, 50, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 50, 50, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 50, 50, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 50, 50, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 50, 50, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 50, 50, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 50, 50, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 50, 50, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 50, 50, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 50, 50, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 50, 50, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 50, 50, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 50, 50, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 50, 50, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 25, 25, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 25, 25, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 25, 25, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 25, 25, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 25, 25, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 25, 25, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 25, 25, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 25, 25, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 25, 25, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 25, 25, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 25, 25, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 25, 25, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 25, 25, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 25, 25, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 25, 25, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 25, 25, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 25, 25, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 25, 25, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 25, 25, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 25, 25, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 25, 25, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 25, 25, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 25, 25, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 25, 25, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 25, 25, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 25, 25, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 25, 25, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 25, 25, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 25, 25, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 25, 25, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 25, 25, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 25, 25, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 25, 25, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 25, 25, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 13, 13, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 13, 13, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 13, 13, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 13, 13, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 13, 13, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 13, 13, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 13, 13, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 13, 13, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 13, 13, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 13, 13, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 13, 13, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 13, 13, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 13, 13, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 13, 13, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 13, 13, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 13, 13, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 13, 13, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 13, 13, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 13, 13, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 13, 13, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 13, 13, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 13, 13, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 13, 13, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 13, 13, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 13, 13, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 13, 13, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 13, 13, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 13, 13, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 13, 13, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 13, 13, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 13, 13, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 13, 13, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 13, 13, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 13, 13, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 13, 13, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 27)           55323       dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,636,763\n",
            "Trainable params: 23,583,643\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "106/106 [==============================] - 19s 183ms/step - loss: 4.0171 - accuracy: 0.1165 - val_loss: 1110.0972 - val_accuracy: 0.0200\n",
            "Epoch 2/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 1.8789 - accuracy: 0.4540 - val_loss: 7.2309 - val_accuracy: 0.1108\n",
            "Epoch 3/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.6188 - accuracy: 0.8150 - val_loss: 0.8058 - val_accuracy: 0.7288\n",
            "Epoch 4/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.3114 - accuracy: 0.9012 - val_loss: 0.6543 - val_accuracy: 0.7948\n",
            "Epoch 5/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.2436 - accuracy: 0.9330 - val_loss: 0.1621 - val_accuracy: 0.9316\n",
            "Epoch 6/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0983 - accuracy: 0.9749 - val_loss: 0.2480 - val_accuracy: 0.9281\n",
            "Epoch 7/20\n",
            "106/106 [==============================] - 18s 166ms/step - loss: 0.0516 - accuracy: 0.9847 - val_loss: 0.1643 - val_accuracy: 0.9517\n",
            "Epoch 8/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.9242 - val_accuracy: 0.8125\n",
            "Epoch 9/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.1056 - accuracy: 0.9690 - val_loss: 0.2439 - val_accuracy: 0.9481\n",
            "Epoch 10/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0468 - accuracy: 0.9844 - val_loss: 0.9305 - val_accuracy: 0.8184\n",
            "Epoch 11/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0443 - accuracy: 0.9855 - val_loss: 0.2000 - val_accuracy: 0.9505\n",
            "Epoch 12/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.0746 - accuracy: 0.9811 - val_loss: 0.1987 - val_accuracy: 0.9458\n",
            "Epoch 13/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.0520 - accuracy: 0.9855 - val_loss: 0.0148 - val_accuracy: 0.9965\n",
            "Epoch 14/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 0.0127 - val_accuracy: 0.9929\n",
            "Epoch 15/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.0076 - val_accuracy: 0.9976\n",
            "Epoch 16/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0185 - accuracy: 0.9962 - val_loss: 0.0051 - val_accuracy: 0.9965\n",
            "Epoch 17/20\n",
            "106/106 [==============================] - 18s 168ms/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 0.0082 - val_accuracy: 0.9941\n",
            "Epoch 18/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0145 - accuracy: 0.9976 - val_loss: 0.0046 - val_accuracy: 0.9976\n",
            "Epoch 19/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 3.5028e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "106/106 [==============================] - 18s 167ms/step - loss: 0.0108 - accuracy: 0.9965 - val_loss: 0.0312 - val_accuracy: 0.9906\n",
            "The time difference is : 374.1830703589999\n",
            "133/133 [==============================] - 7s 51ms/step - loss: 0.0424 - accuracy: 0.9884\n",
            "Accuracy: 98.84379506111145%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH8KHT4csnyH",
        "colab_type": "code",
        "outputId": "614c216e-20a9-47a6-d80f-25b1ce39f8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "recognizer.printme()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.50588235]\n",
            "  [0.52941176]\n",
            "  [0.53333333]\n",
            "  ...\n",
            "  [0.9372549 ]\n",
            "  [0.9372549 ]\n",
            "  [0.9254902 ]]\n",
            "\n",
            " [[0.49019608]\n",
            "  [0.5372549 ]\n",
            "  [0.53333333]\n",
            "  ...\n",
            "  [0.90980392]\n",
            "  [0.92941176]\n",
            "  [0.94117647]]\n",
            "\n",
            " [[0.51764706]\n",
            "  [0.5254902 ]\n",
            "  [0.49411765]\n",
            "  ...\n",
            "  [0.9254902 ]\n",
            "  [0.9372549 ]\n",
            "  [0.9372549 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.61960784]\n",
            "  [0.61176471]\n",
            "  [0.61176471]\n",
            "  ...\n",
            "  [0.05098039]\n",
            "  [0.05098039]\n",
            "  [0.05098039]]\n",
            "\n",
            " [[0.61176471]\n",
            "  [0.61568627]\n",
            "  [0.61176471]\n",
            "  ...\n",
            "  [0.05882353]\n",
            "  [0.05882353]\n",
            "  [0.05490196]]\n",
            "\n",
            " [[0.61960784]\n",
            "  [0.63529412]\n",
            "  [0.62745098]\n",
            "  ...\n",
            "  [0.0627451 ]\n",
            "  [0.0627451 ]\n",
            "  [0.05490196]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgM8SItB1cyN",
        "colab_type": "code",
        "outputId": "43cd9be2-1bde-48ea-d8ae-269ae61fe813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncJYIjDEmw1kiMMaYDPf/AR+WxR3Y/Q9WAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ2NnqvzlONw",
        "colab_type": "code",
        "outputId": "18c61285-1960-4ee3-9d42-73fc54c3e5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}